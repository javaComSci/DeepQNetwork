{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import random, sample\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from collections import deque\n",
    "%run MazeEnv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # hyperparameters\n",
    "        self.episodes = 1028\n",
    "        self.time_allowed_in_game = 500\n",
    "        self.epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon_multiplier = 0.99\n",
    "        self.discount_rate = 0.9\n",
    "\n",
    "        # create 2 models here - one for use with the predictions and the other to train on\n",
    "        # the target model will be set to the trained model after a specfic num of iterations\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        # both models must start with the same weights\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # create the list for the replay memory\n",
    "        self.replay_memory = deque(maxlen=1028)\n",
    "        \n",
    "        # keep track of how often the target_weights = normal_weights\n",
    "        self.t_target_current = 0\n",
    "        self.t_target_threshold = 512\n",
    "        # keep track of how often the to get from minibatch and train\n",
    "        self.t_train_current = 0\n",
    "        self.t_train_threshold = 256\n",
    "        # mini batch to train on\n",
    "        self.mini_batch_size = 64\n",
    "        \n",
    "    \n",
    "    # create the model structure to be used\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # inputs will be the x and y coordinates of the maze - 2 input\n",
    "        model.add(Dense(3, input_dim = 2, activation = \"relu\"))\n",
    "        model.add(Dense(4, activation = \"linear\"))\n",
    "        \n",
    "        # the mse loss works best in dnq\n",
    "        model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # choose an action to perform in the game\n",
    "    def select_action(self, state):\n",
    "        r = random()\n",
    "        # choose a random action\n",
    "        if r < self.epsilon:\n",
    "            # this is coming from the maze file\n",
    "            return select_move_from_num(r)\n",
    "        else:\n",
    "            x = state[0]\n",
    "            y = state[1]\n",
    "            inputs = np.array([x, y]).reshape((1, 2))\n",
    "            move_prediction = np.array(self.model.predict(inputs)[0])\n",
    "            prediction = np.argmax(move_prediction)\n",
    "            return prediction\n",
    "    \n",
    "    # reward + bestaction(for next state) == q(state)\n",
    "    # train the model\n",
    "    def train(self):\n",
    "        print(\"TRAINING\")\n",
    "        # training the model based on the minibatch - check if enough to do the training\n",
    "        if len(self.replay_memory) < self.mini_batch_size:\n",
    "            return\n",
    "        \n",
    "        # get a sample of the replay memory with the minibatch\n",
    "        mini_batch = sample(self.replay_memory, self.mini_batch_size)\n",
    "        \n",
    "        # use the target values for training the model\n",
    "        target_y = []\n",
    "        x = []\n",
    "        \n",
    "        # go through each of the transitions for gradient descent\n",
    "        for i, (old_state, action_direction, reward, new_state, done) in enumerate(mini_batch):\n",
    "            actual = reward\n",
    "            if done == False:\n",
    "                inputs = np.array(new_state).reshape((1, 2))\n",
    "                move_prediction = np.array(self.target_model.predict(inputs)[0])\n",
    "                next_state_q_val = max(move_prediction)\n",
    "                actual = reward + (self.discount_rate * next_state_q_val)\n",
    "            \n",
    "            target_y.append(actual)\n",
    "            x.append(list(old_state))\n",
    "            \n",
    "        target_y = np.array(target_y).reshape((len(target_y), 1))\n",
    "        x = np.array(x).reshape((len(x), 2))\n",
    "        self.model.fit(x, target_y)\n",
    "        \n",
    "        \n",
    "    # run the game multiple times across the env\n",
    "    def run_game(self):\n",
    "        for i in range(self.episodes):\n",
    "            print(f\"EPISODE {i}\")\n",
    "            env = Environment()\n",
    "            \n",
    "            for j in range(self.time_allowed_in_game):\n",
    "                # get the values for the transition tuple to add to replay memory\n",
    "                old_state = (env.current_point_x, env.current_point_y)\n",
    "                action_direction = self.select_action(old_state)\n",
    "                reward = env.move(action_direction)\n",
    "                new_state = (env.current_point_x, env.current_point_y)\n",
    "                done = env.is_done()\n",
    "                self.replay_memory.append((old_state, action_direction, reward, new_state, done))\n",
    "    \n",
    "                # increase the train and target values\n",
    "                self.t_train_current += 1\n",
    "                self.t_target_current += 1\n",
    "                \n",
    "                # update the weights of the train model\n",
    "                if self.t_train_current  % self.t_train_threshold == 0:\n",
    "                    self.train()\n",
    "                \n",
    "                # update the target model with the weight of the trained model\n",
    "                if self.t_target_current % self.t_target_threshold == 0:\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "                \n",
    "                # break out of loop since it is completed - no longer wait on timesteps\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # modify the epsilon value after each episode\n",
    "            # encourage exploration at the beginning then exploitation\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_multiplier, self.min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 0\n",
      "EPISODE 1\n",
      "EPISODE 2\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 878us/step - loss: 5.2449\n",
      "EPISODE 3\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.9766\n",
      "EPISODE 4\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.7084\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 891us/step - loss: 7.5542\n",
      "EPISODE 5\n",
      "EPISODE 6\n",
      "EPISODE 7\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.9798\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 945us/step - loss: 8.0511\n",
      "EPISODE 8\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 8.1060\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 986us/step - loss: 7.4384\n",
      "EPISODE 9\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 10.0443\n",
      "EPISODE 10\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 7.3108\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.6123\n",
      "EPISODE 11\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.9962\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 1.3610\n",
      "EPISODE 12\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 906us/step - loss: 3.6871\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 917us/step - loss: 3.3348\n",
      "EPISODE 13\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 6.5264\n",
      "EPISODE 14\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.3762\n",
      "EPISODE 15\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 906us/step - loss: 5.6621\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 850us/step - loss: 4.5787\n",
      "EPISODE 16\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 4.2048\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 997us/step - loss: 4.3492\n",
      "EPISODE 17\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0729\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 945us/step - loss: 3.8830\n",
      "EPISODE 18\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 933us/step - loss: 4.1191\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.3236\n",
      "EPISODE 19\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 989us/step - loss: 4.0124\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 996us/step - loss: 3.4737\n",
      "EPISODE 20\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 3.0032\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 2.8678\n",
      "EPISODE 21\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 927us/step - loss: 3.1190\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.1805\n",
      "EPISODE 22\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 5.0464\n",
      "EPISODE 23\n",
      "TRAINING\n",
      "2/2 [==============================] - 0s 845us/step - loss: 3.1871\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-189fb8cd758a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-d0a975d0ccd0>\u001b[0m in \u001b[0;36mrun_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# get the values for the transition tuple to add to replay memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mold_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_point_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_point_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0maction_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_point_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_point_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-d0a975d0ccd0>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mmove_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1241\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \"\"\"\n\u001b[0;32m-> 1652\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m   def interleave(self,\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4069\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4070\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4071\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m   4072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m           \u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m   3202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"function\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3204\u001b[0;31m     return tf_decorator.make_decorator(\n\u001b[0m\u001b[1;32m   3205\u001b[0m         \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         Function(\n",
      "\u001b[0;32m~/anaconda3/envs/mypython3/lib/python3.8/site-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \"\"\"\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.run_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.9204836 , -0.647189  , -0.31126982],\n",
       "        [ 1.0514965 ,  0.6012424 , -0.46006852]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([[ 0.10577381, -0.7944095 ,  0.8722613 ,  0.13030863],\n",
       "        [-0.7834987 ,  0.04127634,  0.47940183,  0.37097836],\n",
       "        [ 0.2646073 , -0.15513116, -0.28247386, -0.06056374]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01385774 -0.10407791  0.1142775   0.01707211]\n",
      "(1, 0) 2\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[0. 0. 0. 0.]\n",
      "(0, 0) 0\n",
      "[[ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env_test = Environment()\n",
    "env_test.current_point_x = 1\n",
    "env_test.current_point_y = 1\n",
    "env_test.maze[env_test.current_point_y, env_test.current_point_x] = -1\n",
    "\n",
    "#env_test.end_point_y = 9\n",
    "state = (env_test.current_point_x, env_test.current_point_y)\n",
    "#state = (env_test.end_point_x, env_test.end_point_y)\n",
    "            \n",
    "for j in range(100):\n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "    inputs = np.array([x, y]).reshape((1, 2))\n",
    "    move_prediction = np.array(agent.model.predict(inputs)[0])\n",
    "    print(move_prediction)\n",
    "    prediction_action = np.argmax(move_prediction)\n",
    "    env_test.move(prediction_action)\n",
    "    state = (env_test.current_point_x, env_test.current_point_y)\n",
    "    print(state, prediction_action)\n",
    "    env_test.maze[env_test.current_point_y, env_test.current_point_x] = 1\n",
    "    done = env_test.is_done()\n",
    "    if done:\n",
    "        print(\"DONE\")\n",
    "        break\n",
    "        \n",
    "print(env_test.maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v', '<', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', '<', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', '<', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', '<', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', 'v', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', 'v', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', 'v', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', 'v', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', 'v', '<', '<', '<', '<', '<', '<', '<', '<']\n",
      "['v', 'v', 'v', '<', '<', '<', '<', '<', '<', '<']\n"
     ]
    }
   ],
   "source": [
    "env_test = Environment()\n",
    "\n",
    "for row in range(10):\n",
    "    arr = []\n",
    "    for col in range(10):\n",
    "        inputs = np.array([col, row]).reshape((1, 2))\n",
    "#         print(row, col)\n",
    "        move_prediction = np.array(agent.model.predict(inputs)[0])\n",
    "        prediction_action = np.argmax(move_prediction)\n",
    "        if prediction_action == 0:\n",
    "            arr.append(\"<\")\n",
    "        elif prediction_action == 1:\n",
    "            arr.append(\">\")\n",
    "        elif prediction_action == 2:\n",
    "            arr.append(\"^\")\n",
    "        else:\n",
    "            arr.append(\"v\")\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
