{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-129-cf39d6075b8b>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-129-cf39d6075b8b>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    return self.current_point_x == self.end_point_x && self.current_point_y == self.end_point_y\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-129-cf39d6075b8b>, line 34)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-129-c5ea06e6e6b5>\"\u001b[0m, line \u001b[1;32m7\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_line_magic('run', 'MazeEnv.ipynb')\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2327\u001b[0m, in \u001b[1;35mrun_line_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m\"<decorator-gen-52>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mrun\u001b[0m\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m724\u001b[0m, in \u001b[1;35mrun\u001b[0m\n    self.shell.safe_execfile_ipy(filename, raise_exceptions=True)\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2814\u001b[0m, in \u001b[1;35msafe_execfile_ipy\u001b[0m\n    result.raise_error()\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m329\u001b[0m, in \u001b[1;35mraise_error\u001b[0m\n    raise self.error_before_exec\n",
      "  File \u001b[1;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3117\u001b[0m, in \u001b[1;35mrun_cell_async\u001b[0m\n    code_ast = compiler.ast_parse(cell, filename=cell_name)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/indhu/anaconda3/envs/mypython3/lib/python3.8/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-129-cf39d6075b8b>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    return self.current_point_x == self.end_point_x && self.current_point_y == self.end_point_y\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import random\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from collections import deque\n",
    "%run MazeEnv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # hyperparameters\n",
    "        self.episodes = 100\n",
    "        self.time_allowed_in_game = 100\n",
    "        self.epsilon = 1\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon_multiplier = 0.95\n",
    "        self.discount_rate = 0.9\n",
    "\n",
    "        # create 2 models here - one for use with the predictions and the other to train on\n",
    "        # the target model will be set to the trained model after a specfic num of iterations\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        # both models must start with the same weights\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        # create the list for the replay memory\n",
    "        self.replay_memory = deque(maxlen=1028)\n",
    "        \n",
    "        # keep track of how often the target_weights = normal_weights\n",
    "        self.t_target_current = 0\n",
    "        self.t_target_threshold = 256\n",
    "        # keep track of how often the to get from minibatch and train\n",
    "        self.t_train_current = 0\n",
    "        self.t_train_threshold = 128\n",
    "        # mini batch to train on\n",
    "        self.mini_batch_size = 64\n",
    "        \n",
    "    \n",
    "    # create the model structure to be used\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # inputs will be the x and y coordinates of the maze - 2 input\n",
    "        model.add(Dense(3, input_dim = 2, activation = \"relu\"))\n",
    "        model.add(Dense(4, activation = \"linear\"))\n",
    "        \n",
    "        # the mse loss works best in dnq\n",
    "        model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # choose an action to perform in the game\n",
    "    def select_action(self, state):\n",
    "        r = random()\n",
    "        # choose a random action\n",
    "        if r < 0:\n",
    "            # this is coming from the maze file\n",
    "            return select_move_from_num(r)\n",
    "        else:\n",
    "            x = state[0]\n",
    "            y = state[1]\n",
    "            inputs = np.array([x, y]).reshape((1, 2))\n",
    "            move_prediction = np.array(self.model.predict(inputs)[0])\n",
    "            prediction = np.argmax(move_prediction)\n",
    "            return prediction\n",
    "    \n",
    "    \n",
    "    # train the model\n",
    "    def train(self):\n",
    "        # training the model based on the minibatch - check if enough to do the training\n",
    "        if len(self.replay_memory) < self.mini_batch_size:\n",
    "            return\n",
    "        \n",
    "        # get a sample of the replay memory with the minibatch\n",
    "        mini_batch = random.sample(self.replay_memory(), self.mini_batch_size)\n",
    "        \n",
    "        # use the target values for training the model\n",
    "        target_y = []\n",
    "        x = []\n",
    "        \n",
    "        # go through each of the transitions for gradient descent\n",
    "        for i, (old_state, action_direction, reward, new_state, done) in enumerate(mini_batch):\n",
    "            actual = reward\n",
    "            if done == False:\n",
    "                inputs = np.array(new_state).reshape((1, 2))\n",
    "                move_prediction = np.array(self.target_model.predict(inputs)[0])\n",
    "                next_state_q_val = max(move_prediction)\n",
    "                actual = reward + (self.discount_rate * next_state_q_val)\n",
    "            \n",
    "            target_y.append(actual)\n",
    "            x = list(old_state)\n",
    "            \n",
    "        target_y = np.array(target_array).reshape((len(target_y), 1))\n",
    "        x = np.array(x).reshape((len(x), 2))\n",
    "        self.model.fit(x, target_y)\n",
    "        \n",
    "        \n",
    "    # run the game multiple times across the env\n",
    "    def run_game(self):\n",
    "        for i in range(self.episodes):\n",
    "            env = Environment()\n",
    "            \n",
    "            for j in range(self.time_allowed_in_game):\n",
    "                # get the values for the transition tuple to add to replay memory\n",
    "                old_state = (env.current_point_x, env.current_point_y)\n",
    "                action_direction = self.select_action(old_state)\n",
    "                env.move(action_direction)\n",
    "                new_state = (env.current_point_x, env.current_point_y)\n",
    "                done = env.is_done()\n",
    "                reward = env.get_reward()\n",
    "                print(old_state, action_direction, new_state, reward)\n",
    "                self.replay_memory.append((old_state, action_direction, reward, new_state, done))\n",
    "                \n",
    "                self.t_train_current += 1\n",
    "                self.t_target_current += 1\n",
    "                \n",
    "                if self.t_train_current % self.t_train_threshold == 0:\n",
    "                    self.train()\n",
    "                \n",
    "                # update the target model with the weight of the trained model\n",
    "                if self.t_target_current % self.t_target_threshold == 0:\n",
    "                    self.target_model.set_weights(self.model.get_weights())\n",
    "                \n",
    "                # break out of loop since it is completed - no longer wait on timesteps\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # modify the epsilon value after each episode\n",
    "            # encourage exploration at the beginning then exploitation\n",
    "            self.epsilon = max(self.epsilon * epsilon_multiplier, self.min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Environment' object has no attribute 'is_done'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-189fb8cd758a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-94389c1b5d2c>\u001b[0m in \u001b[0;36mrun_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_point_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_point_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_direction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'is_done'"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.run_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
